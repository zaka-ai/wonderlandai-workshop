{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Weighted_Average_Ensemble",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk0lEBu6wNtY",
        "colab_type": "text"
      },
      "source": [
        "## Weighted Average Ensemble\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysOL_X5v8Aho",
        "colab_type": "text"
      },
      "source": [
        "### Model Averaging Ensemble\n",
        "The results of the model averaging ensemble can be used as a point of comparison as we would expect a well configured weighted average ensemble to perform better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ABVrJC_8A3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model averaging ensemble for the blobs dataset\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import numpy\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "\n",
        "# fit model on dataset\n",
        "def fit_model(trainX, trainy):\n",
        "\ttrainy_enc = to_categorical(trainy)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n",
        "\tmodel.add(Dense(3, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit model\n",
        "\tmodel.fit(trainX, trainy_enc, epochs=500, verbose=0)\n",
        "\treturn model\n",
        "\n",
        "# make an ensemble prediction for multi-class classification\n",
        "def ensemble_predictions(members, testX):\n",
        "\t# make predictions\n",
        "\tyhats = [model.predict(testX) for model in members]\n",
        "\tyhats = array(yhats)\n",
        "\t# sum across ensemble members\n",
        "\tsummed = numpy.sum(yhats, axis=0)\n",
        "\t# argmax across classes\n",
        "\tresult = argmax(summed, axis=1)\n",
        "\treturn result\n",
        "\n",
        "# evaluate a specific number of members in an ensemble\n",
        "def evaluate_n_members(members, n_members, testX, testy):\n",
        "\t# select a subset of members\n",
        "\tsubset = members[:n_members]\n",
        "\t# make prediction\n",
        "\tyhat = ensemble_predictions(subset, testX)\n",
        "\t# calculate accuracy\n",
        "\treturn accuracy_score(testy, yhat)\n",
        "\n",
        "# generate 2d classification dataset\n",
        "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "# split into train and test\n",
        "n_train = 100\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]\n",
        "# fit all models\n",
        "n_members = 10\n",
        "members = [fit_model(trainX, trainy) for _ in range(n_members)]\n",
        "\n",
        "# evaluate different numbers of ensembles on hold out set\n",
        "single_scores, ensemble_scores = list(), list()\n",
        "for i in range(1, len(members)+1):\n",
        "\t# evaluate model with i members\n",
        "\tensemble_score = evaluate_n_members(members, i, testX, testy)\n",
        "\t# evaluate the i'th model standalone\n",
        "\ttesty_enc = to_categorical(testy)\n",
        "\t_, single_score = members[i-1].evaluate(testX, testy_enc, verbose=0)\n",
        "\t# summarize this step\n",
        "\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
        "\tensemble_scores.append(ensemble_score)\n",
        "\tsingle_scores.append(single_score)\n",
        " \n",
        "# summarize average accuracy of a single final model\n",
        "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n",
        "\n",
        "# plot score vs number of ensemble members\n",
        "x_axis = [i for i in range(1, len(members)+1)]\n",
        "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
        "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Qk1NRZ8BXv",
        "colab_type": "text"
      },
      "source": [
        "Now that we know how to develop a model averaging ensemble, we can extend the approach one step further by weighting the contributions of the ensemble members.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fb60v9Sm44a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grid search for coefficients in a weighted average ensemble for the blobs problem\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import tensordot\n",
        "from numpy.linalg import norm\n",
        "from itertools import product\n",
        "\n",
        "# fit model on dataset\n",
        "def fit_model(trainX, trainy):\n",
        "\ttrainy_enc = to_categorical(trainy)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n",
        "\tmodel.add(Dense(3, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit model\n",
        "\tmodel.fit(trainX, trainy_enc, epochs=500, verbose=0)\n",
        "\treturn model\n",
        "\n",
        "# make an ensemble prediction for multi-class classification\n",
        "def ensemble_predictions(members, weights, testX):\n",
        "\t# make predictions\n",
        "\tyhats = [model.predict(testX) for model in members]\n",
        "\tyhats = array(yhats)\n",
        "\t# weighted sum across ensemble members\n",
        "\tsummed = tensordot(yhats, weights, axes=((0),(0)))\n",
        "\t# argmax across classes\n",
        "\tresult = argmax(summed, axis=1)\n",
        "\treturn result\n",
        "\n",
        "# evaluate a specific number of members in an ensemble\n",
        "def evaluate_ensemble(members, weights, testX, testy):\n",
        "\t# make prediction\n",
        "\tyhat = ensemble_predictions(members, weights, testX)\n",
        "\t# calculate accuracy\n",
        "\treturn accuracy_score(testy, yhat)\n",
        "\n",
        "# normalize a vector to have unit norm\n",
        "def normalize(weights):\n",
        "\t# calculate l1 vector norm\n",
        "\tresult = norm(weights, 1)\n",
        "\t# check for a vector of all zeros\n",
        "\tif result == 0.0:\n",
        "\t\treturn weights\n",
        "\t# return normalized vector (unit norm)\n",
        "\treturn weights / result\n",
        "\n",
        "# grid search weights\n",
        "def grid_search(members, testX, testy):\n",
        "\t# define weights to consider\n",
        "\tw = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "\tbest_score, best_weights = 0.0, None\n",
        "\t# iterate all possible combinations (cartesian product)\n",
        "\tfor weights in product(w, repeat=len(members)):\n",
        "\t\t# skip if all weights are equal\n",
        "\t\tif len(set(weights)) == 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# hack, normalize weight vector\n",
        "\t\tweights = normalize(weights)\n",
        "\t\t# evaluate weights\n",
        "\t\tscore = evaluate_ensemble(members, weights, testX, testy)\n",
        "\t\tif score > best_score:\n",
        "\t\t\tbest_score, best_weights = score, weights\n",
        "\t\t\tprint('>%s %.3f' % (best_weights, best_score))\n",
        "\treturn list(best_weights)\n",
        "\n",
        "# generate 2d classification dataset\n",
        "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "\n",
        "# split into train and test\n",
        "n_train = 100\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]\n",
        "\n",
        "# fit all 5 models\n",
        "n_members = 5\n",
        "members = [fit_model(trainX, trainy) for _ in range(n_members)]\n",
        "\n",
        "# evaluate each single model on the test set\n",
        "testy_enc = to_categorical(testy)\n",
        "for i in range(n_members):\n",
        "\t_, test_acc = members[i].evaluate(testX, testy_enc, verbose=0)\n",
        "\tprint('Model %d: %.3f' % (i+1, test_acc))\n",
        " \n",
        "# evaluate averaging ensemble (equal weights)\n",
        "weights = [1.0/n_members for _ in range(n_members)]\n",
        "score = evaluate_ensemble(members, weights, testX, testy)\n",
        "print('Equal Weights Score: %.3f' % score)\n",
        "\n",
        "# grid search weights\n",
        "weights = grid_search(members, testX, testy)\n",
        "score = evaluate_ensemble(members, weights, testX, testy)\n",
        "print('Grid Search Weights: %s, Score: %.3f' % (weights, score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RuN0a-JQp9z",
        "colab_type": "text"
      },
      "source": [
        "## Weighted Average MLP Ensemble\n",
        "An alternative to searching for weight values is to use a directed optimization process. Optimization is a search process, but instead of sampling the space of possible solutions randomly or exhaustively, the search process uses any available information to make the next step in the search, such as toward a set of weights that has lower error.\n",
        "\n",
        "**SciPy** provides an implementation of the **Differential Evolution method**. This is one of the few stochastic global search algorithms that just works for function optimization with continuous inputs, and it works well. The *differential_evolution()* SciPy function requires that a function is specified to evaluate a set of weights and return a score to be minimized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iypbpkikQqQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# global optimization to find coefficients for weighted ensemble on blobs problem\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import tensordot\n",
        "from numpy.linalg import norm\n",
        "from scipy.optimize import differential_evolution\n",
        "\n",
        "# fit model on dataset\n",
        "def fit_model(trainX, trainy):\n",
        "\ttrainy_enc = to_categorical(trainy)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n",
        "\tmodel.add(Dense(3, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit model\n",
        "\tmodel.fit(trainX, trainy_enc, epochs=500, verbose=0)\n",
        "\treturn model\n",
        "\n",
        "# make an ensemble prediction for multi-class classification\n",
        "def ensemble_predictions(members, weights, testX):\n",
        "\t# make predictions\n",
        "\tyhats = [model.predict(testX) for model in members]\n",
        "\tyhats = array(yhats)\n",
        "\t# weighted sum across ensemble members\n",
        "\tsummed = tensordot(yhats, weights, axes=((0),(0)))\n",
        "\t# argmax across classes\n",
        "\tresult = argmax(summed, axis=1)\n",
        "\treturn result\n",
        "\n",
        "# # evaluate a specific number of members in an ensemble\n",
        "def evaluate_ensemble(members, weights, testX, testy):\n",
        "\t# make prediction\n",
        "\tyhat = ensemble_predictions(members, weights, testX)\n",
        "\t# calculate accuracy\n",
        "\treturn accuracy_score(testy, yhat)\n",
        "\n",
        "# normalize a vector to have unit norm\n",
        "def normalize(weights):\n",
        "\t# calculate l1 vector norm\n",
        "\tresult = norm(weights, 1)\n",
        "\t# check for a vector of all zeros\n",
        "\tif result == 0.0:\n",
        "\t\treturn weights\n",
        "\t# return normalized vector (unit norm)\n",
        "\treturn weights / result\n",
        "\n",
        "# We can minimize the classification error (1 - accuracy)\n",
        "# loss function for optimization process, designed to be minimized\n",
        "def loss_function(weights, members, testX, testy):\n",
        "\t# normalize weights\n",
        "\tnormalized = normalize(weights)\n",
        "\t# calculate error rate\n",
        "\treturn 1.0 - evaluate_ensemble(members, normalized, testX, testy)\n",
        "\n",
        "# generate 2d classification dataset\n",
        "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "# split into train and test\n",
        "n_train = 100\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]\n",
        "\n",
        "# fit all models\n",
        "n_members = 5\n",
        "members = [fit_model(trainX, trainy) for _ in range(n_members)]\n",
        "\n",
        "# evaluate each single model on the test set\n",
        "testy_enc = to_categorical(testy)\n",
        "for i in range(n_members):\n",
        "\t_, test_acc = members[i].evaluate(testX, testy_enc, verbose=0)\n",
        "\tprint('Model %d: %.3f' % (i+1, test_acc))\n",
        " \n",
        "# evaluate averaging ensemble (equal weights)\n",
        "weights = [1.0/n_members for _ in range(n_members)]\n",
        "score = evaluate_ensemble(members, weights, testX, testy)\n",
        "print('Equal Weights Score: %.3f' % score)\n",
        "\n",
        "# We can define the bounds as a five-dimensional hypercube (e.g. 5 weights for \n",
        "# the 5 ensemble members) with values between 0.0 and 1.0.\n",
        "# define bounds on each weight\n",
        "bound_w = [(0.0, 1.0)  for _ in range(n_members)]\n",
        "\n",
        "# Our loss function requires three parameters in addition to the weights\n",
        "# arguments to the loss function\n",
        "search_arg = (members, testX, testy)\n",
        "\n",
        "# global optimization of ensemble weights\n",
        "result = differential_evolution(loss_function, bound_w, search_arg, maxiter=10, tol=1e-7)\n",
        "\n",
        "# The results is a dictionary that contains all kinds of information about the search. \n",
        "# The x key contains the optimal set of weights found during the search. \n",
        "# get the chosen weights\n",
        "weights = normalize(result['x'])\n",
        "print('Optimized Weights: %s' % weights)\n",
        "\n",
        "# evaluate chosen weights\n",
        "score = evaluate_ensemble(members, weights, testX, testy)\n",
        "print('Optimized Weights Score: %.3f' % score)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}