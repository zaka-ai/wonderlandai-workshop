{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Resampling_Ensembles",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75smM2QwNW08",
        "colab_type": "text"
      },
      "source": [
        "## MLP Multiclass Classification Problem\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChPXCxHfMmeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# develop an mlp for blobs dataset\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# generate 2d classification dataset\n",
        "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "\n",
        "# one hot encode output variable\n",
        "y = to_categorical(y)\n",
        "\n",
        "# split into train and test\n",
        "n_train = int(0.9 * X.shape[0])\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=2, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
        "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy', pad=-40)\n",
        "pyplot.plot(history.history['acc'], label='train')\n",
        "pyplot.plot(history.history['val_acc'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDie4gTMNg6G",
        "colab_type": "text"
      },
      "source": [
        "## Random Splits Ensemble\n",
        "The instability of the model and the small test dataset mean that we donâ€™t really know how well this model will perform on new data in general. We can try a simple resampling method of repeatedly generating new random splits of the dataset in train and test sets and fit new models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-vpTjOSNhKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# random-splits mlp ensemble on blobs dataset\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import numpy\n",
        "\n",
        "# evaluate a single mlp model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "\t# encode targets\n",
        "\ttrainy_enc = to_categorical(trainy)\n",
        "\ttesty_enc = to_categorical(testy)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(50, input_dim=2, activation='relu'))\n",
        "\tmodel.add(Dense(3, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit model\n",
        "\tmodel.fit(trainX, trainy_enc, epochs=50, verbose=0)\n",
        "\t# evaluate the model\n",
        "\t_, test_acc = model.evaluate(testX, testy_enc, verbose=0)\n",
        "\treturn model, test_acc\n",
        "\n",
        "# make an ensemble prediction for multi-class classification\n",
        "def ensemble_predictions(members, testX):\n",
        "\t# make predictions\n",
        "\tyhats = [model.predict(testX) for model in members]\n",
        "\tyhats = array(yhats)\n",
        "\t# sum across ensemble members\n",
        "\tsummed = numpy.sum(yhats, axis=0)\n",
        "\t# argmax across classes\n",
        "\tresult = argmax(summed, axis=1)\n",
        "\treturn result\n",
        "\n",
        "# evaluate a specific number of members in an ensemble\n",
        "def evaluate_n_members(members, n_members, testX, testy):\n",
        "\t# select a subset of members\n",
        "\tsubset = members[:n_members]\n",
        "\t# make prediction\n",
        "\tyhat = ensemble_predictions(subset, testX)\n",
        "\t# calculate accuracy\n",
        "\treturn accuracy_score(testy, yhat)\n",
        "\n",
        "# generate 2d classification dataset\n",
        "dataX, datay = make_blobs(n_samples=55000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "X, newX = dataX[:5000, :], dataX[5000:, :]\n",
        "y, newy = datay[:5000], datay[5000:]\n",
        "\n",
        "# multiple train-test splits\n",
        "n_splits = 10\n",
        "scores, members = list(), list()\n",
        "for _ in range(n_splits):\n",
        "\t# split data\n",
        "\ttrainX, testX, trainy, testy = train_test_split(X, y, test_size=0.10)\n",
        "\t# evaluate model\n",
        "\tmodel, test_acc = evaluate_model(trainX, trainy, testX, testy)\n",
        "\tprint('>%.3f' % test_acc)\n",
        "\tscores.append(test_acc)\n",
        "\tmembers.append(model)\n",
        " \n",
        "# After fitting and evaluating the models, we can estimate the expected performance \n",
        "# of a given model with the chosen configuration for the domain. \n",
        "# summarize expected performance\n",
        "print('Estimated Accuracy %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "\n",
        "# evaluate different numbers of ensembles on hold out set\n",
        "single_scores, ensemble_scores = list(), list()\n",
        "for i in range(1, n_splits+1):\n",
        "\tensemble_score = evaluate_n_members(members, i, newX, newy)\n",
        "\tnewy_enc = to_categorical(newy)\n",
        "\t_, single_score = members[i-1].evaluate(newX, newy_enc, verbose=0)\n",
        "\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
        "\tensemble_scores.append(ensemble_score)\n",
        "\tsingle_scores.append(single_score)\n",
        " \n",
        "# plot score vs number of ensemble members\n",
        "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n",
        "x_axis = [i for i in range(1, n_splits+1)]\n",
        "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
        "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxxnXTwLcH_N",
        "colab_type": "text"
      },
      "source": [
        "## Cross-Validation Ensemble\n",
        "A problem with repeated random splits as a resampling method for estimating the average performance of model is that it is optimistic. An approach designed to be less optimistic and is widely used as a result is the k-fold cross-validation method. The method is less biased because each example in the dataset is only used one time in the test dataset to estimate model performance, unlike random train-test splits where a given example may be used to evaluate a model many times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkIYDDYPcTuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cross-validation mlp ensemble on blobs dataset\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import numpy\n",
        "\n",
        "# evaluate a single mlp model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "\t# encode targets\n",
        "\ttrainy_enc = to_categorical(trainy)\n",
        "\ttesty_enc = to_categorical(testy)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(50, input_dim=2, activation='relu'))\n",
        "\tmodel.add(Dense(3, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit model\n",
        "\tmodel.fit(trainX, trainy_enc, epochs=50, verbose=0)\n",
        "\t# evaluate the model\n",
        "\t_, test_acc = model.evaluate(testX, testy_enc, verbose=0)\n",
        "\treturn model, test_acc\n",
        "\n",
        "# make an ensemble prediction for multi-class classification\n",
        "def ensemble_predictions(members, testX):\n",
        "\t# make predictions\n",
        "\tyhats = [model.predict(testX) for model in members]\n",
        "\tyhats = array(yhats)\n",
        "\t# sum across ensemble members\n",
        "\tsummed = numpy.sum(yhats, axis=0)\n",
        "\t# argmax across classes\n",
        "\tresult = argmax(summed, axis=1)\n",
        "\treturn result\n",
        "\n",
        "# evaluate a specific number of members in an ensemble\n",
        "def evaluate_n_members(members, n_members, testX, testy):\n",
        "\t# select a subset of members\n",
        "\tsubset = members[:n_members]\n",
        "\t# make prediction\n",
        "\tyhat = ensemble_predictions(subset, testX)\n",
        "\t# calculate accuracy\n",
        "\treturn accuracy_score(testy, yhat)\n",
        "\n",
        "# generate 2d classification dataset\n",
        "dataX, datay = make_blobs(n_samples=55000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "X, newX = dataX[:5000, :], dataX[5000:, :]\n",
        "y, newy = datay[:5000], datay[5000:]\n",
        "\n",
        "# prepare the k-fold cross-validation configuration\n",
        "n_folds = 10\n",
        "kfold = KFold(n_folds, True, 1)\n",
        "\n",
        "# cross validation estimation of performance\n",
        "scores, members = list(), list()\n",
        "for train_ix, test_ix in kfold.split(X):\n",
        "\t# select samples\n",
        "\ttrainX, trainy = X[train_ix], y[train_ix]\n",
        "\ttestX, testy = X[test_ix], y[test_ix]\n",
        "\t# evaluate model\n",
        "\tmodel, test_acc = evaluate_model(trainX, trainy, testX, testy)\n",
        "\tprint('>%.3f' % test_acc)\n",
        "\tscores.append(test_acc)\n",
        "\tmembers.append(model)\n",
        " \n",
        "# summarize expected performance\n",
        "print('Estimated Accuracy %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "\n",
        "# evaluate different numbers of ensembles on hold out set\n",
        "single_scores, ensemble_scores = list(), list()\n",
        "for i in range(1, n_folds+1):\n",
        "\tensemble_score = evaluate_n_members(members, i, newX, newy)\n",
        "\tnewy_enc = to_categorical(newy)\n",
        "\t_, single_score = members[i-1].evaluate(newX, newy_enc, verbose=0)\n",
        "\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
        "\tensemble_scores.append(ensemble_score)\n",
        "\tsingle_scores.append(single_score)\n",
        " \n",
        "# plot score vs number of ensemble members\n",
        "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n",
        "x_axis = [i for i in range(1, n_folds+1)]\n",
        "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
        "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O16bzBG3eS0d",
        "colab_type": "text"
      },
      "source": [
        "## Bagging Ensemble\n",
        "A limitation of random splits and k-fold cross-validation from the perspective of ensemble learning is that the models are very similar.\n",
        "\n",
        "Generally, use of the bootstrap method in ensemble learning is referred to as bootstrap aggregation or ***bagging***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94vM91IJes1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bagging mlp ensemble on blobs dataset\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import numpy\n",
        "\n",
        "# evaluate a single mlp model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "\t# encode targets\n",
        "\ttrainy_enc = to_categorical(trainy)\n",
        "\ttesty_enc = to_categorical(testy)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(50, input_dim=2, activation='relu'))\n",
        "\tmodel.add(Dense(3, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit model\n",
        "\tmodel.fit(trainX, trainy_enc, epochs=50, verbose=0)\n",
        "\t# evaluate the model\n",
        "\t_, test_acc = model.evaluate(testX, testy_enc, verbose=0)\n",
        "\treturn model, test_acc\n",
        "\n",
        "# make an ensemble prediction for multi-class classification\n",
        "def ensemble_predictions(members, testX):\n",
        "\t# make predictions\n",
        "\tyhats = [model.predict(testX) for model in members]\n",
        "\tyhats = array(yhats)\n",
        "\t# sum across ensemble members\n",
        "\tsummed = numpy.sum(yhats, axis=0)\n",
        "\t# argmax across classes\n",
        "\tresult = argmax(summed, axis=1)\n",
        "\treturn result\n",
        "\n",
        "# evaluate a specific number of members in an ensemble\n",
        "def evaluate_n_members(members, n_members, testX, testy):\n",
        "\t# select a subset of members\n",
        "\tsubset = members[:n_members]\n",
        "\t# make prediction\n",
        "\tyhat = ensemble_predictions(subset, testX)\n",
        "\t# calculate accuracy\n",
        "\treturn accuracy_score(testy, yhat)\n",
        "\n",
        "# generate 2d classification dataset\n",
        "dataX, datay = make_blobs(n_samples=55000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "X, newX = dataX[:5000, :], dataX[5000:, :]\n",
        "y, newy = datay[:5000], datay[5000:]\n",
        "\n",
        "# multiple train-test splits\n",
        "n_splits = 10\n",
        "scores, members = list(), list()\n",
        "for _ in range(n_splits):\n",
        "\t# select indexes\n",
        "\tix = [i for i in range(len(X))]\n",
        "\ttrain_ix = resample(ix, replace=True, n_samples=4500)\n",
        "\ttest_ix = [x for x in ix if x not in train_ix]\n",
        "\t# select data\n",
        "\ttrainX, trainy = X[train_ix], y[train_ix]\n",
        "\ttestX, testy = X[test_ix], y[test_ix]\n",
        "\t# evaluate model\n",
        "\tmodel, test_acc = evaluate_model(trainX, trainy, testX, testy)\n",
        "\tprint('>%.3f' % test_acc)\n",
        "\tscores.append(test_acc)\n",
        "\tmembers.append(model)\n",
        " \n",
        "# summarize expected performance\n",
        "print('Estimated Accuracy %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "# evaluate different numbers of ensembles on hold out set\n",
        "single_scores, ensemble_scores = list(), list()\n",
        "for i in range(1, n_splits+1):\n",
        "\tensemble_score = evaluate_n_members(members, i, newX, newy)\n",
        "\tnewy_enc = to_categorical(newy)\n",
        "\t_, single_score = members[i-1].evaluate(newX, newy_enc, verbose=0)\n",
        "\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
        "\tensemble_scores.append(ensemble_score)\n",
        "\tsingle_scores.append(single_score)\n",
        " \n",
        "# plot score vs number of ensemble members\n",
        "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n",
        "x_axis = [i for i in range(1, n_splits+1)]\n",
        "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
        "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}