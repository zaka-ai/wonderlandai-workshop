{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00_MLP_Variance",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-HQR4-ajalr",
        "colab_type": "text"
      },
      "source": [
        "# Multiclass classfication problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTkJ6usWjXtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scatter plot of blobs dataset\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "\n",
        "# generate 2d classification dataset\n",
        "X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "\n",
        "# scatter plot for each class value\n",
        "for class_value in range(3):\n",
        "\t# select indices of points with the class label\n",
        "\trow_ix = where(y == class_value)\n",
        "\t# scatter plot for points with a different color\n",
        "\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
        "\n",
        "# show plot\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D-fc3fDj5Zv",
        "colab_type": "text"
      },
      "source": [
        "## MLP Model for Multiclass Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLl5ES4Jj5jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit high variance mlp on blobs classification problem\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# generate 2d classification dataset\n",
        "X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "# convert to one-hot encoding\n",
        "y = to_categorical(y)\n",
        "\n",
        "# split into train and test\n",
        "n_train = int(0.3 * X.shape[0])\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(15, input_dim=2, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
        "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot loss learning curves\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "\n",
        "# plot accuracy learning curves\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy', pad=-40)\n",
        "pyplot.plot(history.history['acc'], label='train')\n",
        "pyplot.plot(history.history['val_acc'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ai1Rb-GlD1Z",
        "colab_type": "text"
      },
      "source": [
        "## High Variance of MLP Model\n",
        "\n",
        "It is important to demonstrate that the model indeed has a variance in its prediction. We can demonstrate this by repeating the fit and evaluation of the same model configuration on the same dataset and summarizing the final performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ak4_v_GlEq3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# demonstrate high variance of mlp model on blobs classification problem\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# fit and evaluate a neural net model on the dataset\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(15, input_dim=2, activation='relu'))\n",
        "\tmodel.add(Dense(3, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit model\n",
        "\tmodel.fit(trainX, trainy, epochs=200, verbose=0)\n",
        "\t# evaluate the model\n",
        "\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
        "\treturn test_acc\n",
        "\n",
        "# generate 2d classification dataset\n",
        "X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "y = to_categorical(y)\n",
        "\n",
        "# split into train and test\n",
        "n_train = int(0.3 * X.shape[0])\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]\n",
        "\n",
        "# repeated evaluation\n",
        "n_repeats = 30\n",
        "scores = list()\n",
        "for _ in range(n_repeats):\n",
        "\tscore = evaluate_model(trainX, trainy, testX, testy)\n",
        "\tprint('> %.3f' % score)\n",
        "\tscores.append(score)\n",
        " \n",
        "# summarize the distribution of scores\n",
        "print('Scores Mean: %.3f, Standard Deviation: %.3f' % (mean(scores), std(scores)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}